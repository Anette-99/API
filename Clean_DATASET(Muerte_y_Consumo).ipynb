{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_DATASET(Muerte y Consumo).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMd9I+gyiE2kqF4mCznJSs1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anette-99/API/blob/master/Clean_DATASET(Muerte_y_Consumo).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MLFOzsk9n63"
      },
      "source": [
        "**1.-** Primero Instalamos la paqueteria de pySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQDAVfk1JpV"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87KKUMf9DeMq"
      },
      "source": [
        "# **Muertes por sobredosis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3guMDmfg9peJ"
      },
      "source": [
        "**2.-** Leer el csv. Crear un objeto de tipo DataFrameReader usando nuestra propiedad spark.read Creamos un objeto de lectura especificando los Headers.\n",
        "Importamos SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUQvt3vE1QRK"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "Muertes = spark.read.csv(\"Muertes_sucio.csv\")\n",
        "Muertes.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkGdFcgU9qwv"
      },
      "source": [
        "**3.-** Para poder especificar los nombres de columnas debemos de colocar el siguiente codigo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvp67Y4Y1QY8"
      },
      "source": [
        "Muertes = spark.read.options(header=\"true\").csv(\"Muertes_sucio.csv\")\n",
        "Muertes.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DKnYp0Z9riu"
      },
      "source": [
        "**4.-** Ahora vamos a mostrar los tipos de datos que tiene el Dataset. Lo que debemos de hacer prmero es importar \"pprint\", lo que nos indicara que tipo de datos hay en el csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MREkEwhD1Qeb"
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(Muertes.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w1IslA29sWK"
      },
      "source": [
        "**5.-** Definimos el esquema de tipos de datos adecuado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u__0K9WY1QhK"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "schema = StructType([StructField(\"ID\", IntegerType(), nullable=False), \n",
        "                     StructField(\"FECHA\", DateType(), nullable=False), \n",
        "                     StructField(\"FECHA DE INFORME: 1, FECHA DE FALLECIMIENTO: 0\", IntegerType(), nullable=False), \n",
        "                     StructField(\"EDAD\", ByteType(), nullable=True), \n",
        "                     StructField(\"SEXO\", StringType(), nullable=False), \n",
        "                     StructField(\"RAZA\", StringType(), nullable=True), \n",
        "                     StructField(\"DESCRIPCI�N DE LA LESI�N\", StringType(), nullable=False), \n",
        "                     StructField(\"CAUSA DE LA MUERTE\", StringType(), nullable=False),\n",
        "                     StructField(\"OTROS FACTORES SIGNIFICATIVO\", StringType(), nullable=False), \n",
        "                     StructField(\"HEROINA\", IntegerType(), nullable=True), \n",
        "                     StructField(\"COCAINA\", IntegerType(), nullable=True), \n",
        "                     StructField(\"FENTANILO\", IntegerType(), nullable=True), \n",
        "                     StructField(\"ETANOL\", IntegerType(), nullable=True), \n",
        "                     StructField(\"Manera de morir\", StringType(), nullable=True)])\n",
        "#Muertes1 = spark.read.schema(schema).options(header=\"true\").csv(\"Muertes_sucio.csv\")\n",
        "Muertes1 = spark.read.options(header=\"true\").schema(schema).csv(\"Muertes_sucio.csv\")\n",
        "pprint(Muertes1.dtypes)\n",
        "Muertes1.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyE-rWT79tep"
      },
      "source": [
        "**6.-** Eliminar las filas erróneas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHtsgz7g1Qjk"
      },
      "source": [
        "Muertes1RowRemoved = spark.read.schema(schema).options(header=\"true\", mode = \"DROPMALFORMED\").csv(\"Muertes_sucio.csv\")\n",
        "Muertes1RowRemoved.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-VCBQhb9uKI"
      },
      "source": [
        "**7.-** Este codigo va despues de nuestro esquema, para corregir y visualizar la fecha correctamente.\n",
        "Podemos visualizar que de este modo estamos limpiando el Datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YuOFf6V1Ql-"
      },
      "source": [
        "MuertesRowsRemoved = spark.read.schema(schema).options(header=\"true\", mode = \"DROPMALFORMED\", dateFormat = \"dd/MM/yyyy\").csv(\"Muertes_sucio.csv\")\n",
        "pprint(MuertesRowsRemoved.dtypes)\n",
        "MuertesRowsRemoved.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkoT0dtP9vFC"
      },
      "source": [
        "**8.-** Rellenar valores vacios.\n",
        "\n",
        "Con la palabra reservada fillna rellenamos las celdas que aparecen vacias con un valor. En mi caso tuve que copiar ne nuevo el código para rellenar otra columna diferente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVk1Jsxf2EPy"
      },
      "source": [
        "MN123= spark.read.schema(schema).options(header=\"true\", dateFormat = \"dd/MM/yyyy\").csv(\"Muertes_sucio.csv\")\n",
        "MN123.show()\n",
        "MuerteNew= MN123.fillna({ \"FECHA DE INFORME: 1, FECHA DE FALLECIMIENTO: 0\":0,\"EDAD\":23,\"SEXO\": \"Female\" ,\"RAZA\": \"White\", \"OTROS FACTORES SIGNIFICATIVO\": \"No\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edBB3hbZ9wSw"
      },
      "source": [
        "**9.-** Visualizamos los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4yJlu-b2Eet"
      },
      "source": [
        "MuerteNew.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC5zI2z49xEN"
      },
      "source": [
        "**10.-** Descargamos el archivo de manera que podamos volverlo a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCLi4cpg2ElR"
      },
      "source": [
        "MuerteNew.write.options(header=\"true\").csv(\"MuerteNew.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WWqah0a9xy5"
      },
      "source": [
        "**11.-** Utilizar condiciones para afectar valores especificos.\n",
        "Aqui afectaremos la fecha:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8qJrxeF_YYw"
      },
      "source": [
        "MNew = spark.read.schema(schema).options(header=\"true\", dateFormat = \"dd/MM/yyyy\").csv(\"MuerteNew.csv\")\n",
        "MNew.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLfWihuQ_Yz8"
      },
      "source": [
        "**12.-** Aqui le vamos a incrementar 1 año: Utilizando el date.today significa que estamos por utilizar la fecha de hoy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RouClpDA_ZIy"
      },
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "from datetime import date\n",
        "one_year_from_now = date.today().replace(year=date.today().year + 1)\n",
        "New = MNew.withColumn(\"FECHA\", when(col(\"FECHA\")> one_year_from_now, None).otherwise(col(\"FECHA\")))\n",
        "pprint(New.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u78lhlCemILz"
      },
      "source": [
        "**13.-** Podemos visualizar que datos son los que son superiores al año actual:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jYNG02olHrs"
      },
      "source": [
        "New.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ik1bTao_ZeK"
      },
      "source": [
        "**14.-** Ahora eliminamos las filas que contiene valores nulos de nuestra tabla anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeObr0rwgD-_"
      },
      "source": [
        "MN1234remove = New.dropna()\n",
        "MN1234remove.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIrgz89ygEaR"
      },
      "source": [
        "**15.-** Procedemos a descargar nuestro dataset limpio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJWZOPqFgEsh"
      },
      "source": [
        "MN1234remove.write.options(header=\"true\").csv(\"Muerte_Limpia.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUrmtjtAno7w"
      },
      "source": [
        "------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plQ_JzgtEIbE"
      },
      "source": [
        "# **Consumo de drogas por edad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGVhmr4xEOQx"
      },
      "source": [
        "**1.-** Leer el csv. Crear un objeto de tipo DataFrameReader usando nuestra propiedad spark.read Creamos un objeto de lectura especificando los Headers. Importamos SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vyW4OxMESTU"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "Consumo = spark.read.csv(\"Consumo_sucio.csv\")\n",
        "Consumo.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U21_VB-HEZdH"
      },
      "source": [
        "**2.-** Para poder especificar los nombres de columnas debemos de colocar el siguiente codigo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E0ThwMAEZqb"
      },
      "source": [
        "Consumo = spark.read.options(header=\"true\").csv(\"Consumo_sucio.csv\")\n",
        "Consumo.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1j1EEEZ2h"
      },
      "source": [
        "**3.-** Ahora vamos a mostrar los tipos de datos que tiene el Dataset. Lo que debemos de hacer prmero es importar \"pprint\", lo que nos indicara que tipo de datos hay en el csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9wT1YxEaCg"
      },
      "source": [
        "from pprint import pprint\n",
        "pprint(Consumo.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1yVlFjEEaPZ"
      },
      "source": [
        "**4.-** Definimos el esquema de tipos de datos adecuado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJqM5AKHEab9"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "schema = StructType([StructField(\"EDAD\", IntegerType(), nullable=False),\n",
        "                     StructField(\"FECHA\", DateType(), nullable=True),  \n",
        "                     StructField(\"No PERSONAS\", ByteType(), nullable=False), \n",
        "                     StructField(\"Uso de alcohol\",FloatType(), nullable=False),\n",
        "                     StructField(\"Frecuencia de alcohol\", FloatType(), nullable=False), \n",
        "                     StructField(\"Uso de marihuana\", FloatType(), nullable=False), \n",
        "                     StructField(\"Frecuencia de la marihuana\", FloatType(), nullable=False), \n",
        "                     StructField(\"Uso de coca�na\", FloatType(), nullable=False), \n",
        "                     StructField(\"Frecuencia de coca�na\", FloatType(), nullable=False),\n",
        "                     StructField(\"Uso de grietas\", FloatType(), nullable=False), \n",
        "                     StructField(\"Frecuencia de grietas\", FloatType(), nullable=False), \n",
        "                     StructField(\"Uso de hero�na\", FloatType(), nullable=False), \n",
        "                     StructField(\"Frecuencia de hero�na\", FloatType(), nullable=False)])\n",
        "\n",
        "Consumo = spark.read.options(header=\"true\").schema(schema).csv(\"Consumo_sucio.csv\")\n",
        "pprint(Consumo.dtypes)\n",
        "Consumo.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbEBrChTEans"
      },
      "source": [
        "**5.-** Eliminar las filas erróneas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atGrGrkgEay7"
      },
      "source": [
        "CONS1RowsRemoved = spark.read.schema(schema).options(header=\"true\", mode = \"DROPMALFORMED\").csv(\"Consumo_sucio.csv\")\n",
        "CONS1RowsRemoved.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaxtYkVSE-di"
      },
      "source": [
        "**6.-** Este codigo va despues de nuestro esquema, para corregir y visualizar la fecha correctamente. Podemos visualizar que de este modo estamos limpiando el Datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krt4Oj_oE-ry"
      },
      "source": [
        "CONS1RowsRemoved = spark.read.schema(schema).options(header=\"true\", mode = \"DROPMALFORMED\", dateFormat = \"dd/MM/yyyy\").csv(\"Consumo_sucio.csv\")\n",
        "pprint(CONS1RowsRemoved.dtypes)\n",
        "CONS1RowsRemoved.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9eY7AAE-4c"
      },
      "source": [
        "**7.-** Rellenar valores vacios.\n",
        "Con la palabra reservada **fillna** rellenamos las celdas que aparecen vacias con un valor. En mi caso tuve que copiar ne nuevo el código para rellenar otra columna diferente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tUoOhkOE_FG"
      },
      "source": [
        "CN123= spark.read.schema(schema).options(header=\"true\", dateFormat = \"dd/MM/yyyy\").csv(\"Consumo_sucio.csv\")\n",
        "CN123.show()\n",
        "ConsumoNew= CN123.fillna({ \"Uso de grietas\":2.8,\"Frecuencia de grietas\":2.3, \"Uso de hero�na\":0,\"Frecuencia de hero�na\":3.1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMD6092SLVGD"
      },
      "source": [
        "**8.-** Visualizamos los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw3SSuR4LVZ_"
      },
      "source": [
        "ConsumoNew.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj2emBmFL828"
      },
      "source": [
        "**9.-** Descargamos el archivo de manera que podamos volverlo a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8gZQ0bGoZgl"
      },
      "source": [
        "ConsumoNew.write.options(header=\"true\").csv(\"ConsumoNew.csv\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkcqn--6SklV"
      },
      "source": [
        "**10.-** Utilizar condiciones para afectar valores especificos. Aqui afectaremos la fecha:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ih22X8BSk9K"
      },
      "source": [
        "CNew = spark.read.schema(schema).options(header=\"true\", dateFormat = \"dd/MM/yyyy\").csv(\"ConsumoNew.csv\")\n",
        "CNew.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st_2ToMHSl6A"
      },
      "source": [
        "**11.-** Aqui le vamos a incrementar 1 año: Utilizando el date.today significa que estamos por utilizar la fecha de hoy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SJ02z7TSmO0"
      },
      "source": [
        "from pyspark.sql.functions import col, when \n",
        "from datetime import date\n",
        "one_year_from_now = date.today().replace(year=date.today().year + 1)\n",
        "New = CNew.withColumn(\"FECHA\", when(col(\"FECHA\")> one_year_from_now, None).otherwise(col(\"FECHA\")))\n",
        "pprint(New.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ff8-qhwjS_K"
      },
      "source": [
        "**12.-** Visualizamos los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2KqZbbrjTRy"
      },
      "source": [
        "New.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnESWs_aOK4L"
      },
      "source": [
        "**13.-** Aqui solo aplicamos las columnas que ocuparemos con sus datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR8kOpK2OK_D"
      },
      "source": [
        "Consumo_Limp = New.select(\"EDAD\", \"No PERSONAS\", \"Uso de alcohol\", \"Frecuencia de alcohol\", \"Uso de marihuana\", \"Frecuencia de la marihuana\", \"Uso de coca�na\", \"Frecuencia de coca�na\", \"Uso de grietas\", \"Frecuencia de grietas\", \"Uso de hero�na\", \"Frecuencia de hero�na\")\n",
        "pprint(Consumo_Limp.dtypes)\n",
        "Consumo_Limp.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xNhHw7xQwAr"
      },
      "source": [
        "**14.-** Procedemos a descargar nuestra data limpia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4cl5AodQwMV"
      },
      "source": [
        "Consumo_Limp.write.options(header=\"true\").csv(\"Consumo_Limpia.csv\")"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}